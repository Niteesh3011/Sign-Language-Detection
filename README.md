ğŸ“š Sign Language Detection System

Real-Time ASL/ISL Recognition using MediaPipe & Random Forests

ğŸ§­ Overview

This project bridges communication gaps between the Deaf & Hard of Hearing (DHH) community and the hearing population.
It translates American Sign Language (ASL) and Indian Sign Language (ISL) hand gestures to text (and speech) using real-time computer vision.

Unlike expensive sensor-based gloves or GPU-dependent CNNs, this system operates fully on CPU using:

MediaPipe for 21-landmark hand tracking

Random Forest Classifier for high-accuracy predictions

ğŸ” Why This Approach?

Traditional systems rely on:

Flex-sensor gloves (expensive, non-portable)

Depth sensors (hardware dependent)

CNNs (heavy computation, large datasets)

This project:

Uses lightweight mathematical landmark geometry

Requires only tabular features (42 values)

Runs smoothly on low-end laptops

ğŸ§  Core Principles
ğŸ– MediaPipe Hand Tracking

Detects 21 3D hand landmarks

Works in real time using BlazePalm + Landmark Model

Bounding box reused to speed future frames

ğŸŒ² Random Forest Classification

Classifies gestures based on 42 normalized coordinates

Fast CPU inference

Stable and interpretable model

ğŸ—‚ Project Structure
Sign-Language-Detector/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ B/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ data.pickle
â”‚   â””â”€â”€ model.p
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ app.py
â”œâ”€â”€ collect_imgs.py
â”œâ”€â”€ create_dataset.py
â”œâ”€â”€ train_classifier.py
â”œâ”€â”€ inference_classifier.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ›  Technology Stack
Technology	Purpose
OpenCV	Camera input, visualization
MediaPipe	Landmark detection
Scikit-Learn	Random Forest classification
Flask	Web-based deployment
NumPy	Numerical feature manipulation
Pickle	Model/data serialization
ğŸ“¥ Installation
git clone https://github.com/<your-username>/sign-language-detector.git
cd sign-language-detector
pip install -r requirements.txt

ğŸ“¸ Phase I â€” Data Collection

Run:

python collect_imgs.py


You will:

Show gestures to webcam

Press Q to start automatic capture

~100 images per class recommended

âš™ Phase II â€” Feature Extraction

Run:

python create_dataset.py


This:

Extracts 21 landmarks

Converts them to 42-D geometry

Saves data.pickle

ğŸ¤– Phase III â€” Training Model

Run:

python train_classifier.py


Outputs:

Training accuracy

Stores model as model.p

ğŸ— Phase IV â€” Real-Time Detection

Run:

python inference_classifier.py


Displays:

Hand bounding box

Predicted sign label

Latency: 20â€“30 FPS on CPU

ğŸŒ Flask Web Deployment

Run:

python app.py


Browser:

http://127.0.0.1:5000/

ğŸ“Š How It Works â€” Simple Explanation

Camera feeds frames

MediaPipe extracts 21 landmarks

Coordinates are normalized

42-value vector goes to Random Forest

Forest votes â†’ predicted sign

Display result

ğŸš€ Performance & Insights
Why Tabularization Works

Rather than classify millions of pixels, we classify 42 precise geometry values.

This is:

Robust

Lightweight

Deployable anywhere

Latency Optimization

Bounding box reused

Model inference cost < 1 ms

Smooth webcam rendering

ğŸ§­ Limitations & Future Enhancements
Planned Improvements:

âœ” Dynamic sign recognition (LSTMs/GRUs)
âœ” 2-hand gesture support
âœ” NLP auto-correction
âœ” Mobile edge deployment (ONNX/TFLite)

ğŸ¥‡ Conclusion

This project proves that real-time sign recognition does not require deep learning or GPUs.

It brings:

Accuracy

Accessibility

Deployability

A meaningful step toward inclusive communication.

ğŸ“¬ Contribution

Pull Requests welcome!
If youâ€™d like dataset access or improved models, open an issue.

ğŸ“œ License

MIT License.

ğŸ‘¤ Author

Niteesh Pandit
